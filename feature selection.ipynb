{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d361031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel, RFE, chi2, SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# =========================================================================\n",
    "# 1. METHOD: FEATURE IMPORTANCE (RANDOM FOREST) for feature selection part\n",
    "\n",
    "# Train a Random Forest Classifier to calculate importance scores\n",
    "rf_selector = RandomForestClassifier(random_state=42)\n",
    "rf_selector.fit(X_train, y_train)\n",
    "\n",
    "# Create a Series of feature importances\n",
    "importance_df = pd.Series(\n",
    "    rf_selector.feature_importances_, \n",
    "    index=X_train.columns\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "print(\"--- 1. Random Forest Feature Importance (Top 10) ---\")\n",
    "print(importance_df.head(10))\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 2. METHOD: RECURSIVE FEATURE ELIMINATION (RFE)\n",
    "\n",
    "# Initialize a model (Logistic Regression) to use within RFE\n",
    "rfe_model = LogisticRegression(solver='liblinear', random_state=42)\n",
    "\n",
    "# Initialize RFE to select the top 10 features\n",
    "rfe_selector = RFE(estimator=rfe_model, n_features_to_select=10, step=1)\n",
    "rfe_selector.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected features\n",
    "rfe_selected_features = X_train.columns[rfe_selector.support_]\n",
    "\n",
    "print(\"\\n--- 2. RFE Selected Features (Top 10) ---\")\n",
    "print(rfe_selected_features.tolist())\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 3. METHOD: CHI-SQUARE TEST\n",
    "\n",
    "# Note: Chi-Square requires non-negative data. Our scaled continuous features\n",
    "# are both negative and positive, so we'll only apply this to the binary (0/1)\n",
    "# one-hot encoded features which are guaranteed non-negative.\n",
    "\n",
    "# Identify the categorical/binary columns\n",
    "binary_cols = X_train.columns[~X_train.columns.isin(numerical_cols)]\n",
    "\n",
    "# Apply SelectKBest with Chi-Square (selecting the top 10 binary features)\n",
    "chi2_selector = SelectKBest(chi2, k=10)\n",
    "chi2_selector.fit(X_train[binary_cols], y_train)\n",
    "\n",
    "# Get the selected features\n",
    "chi2_selected_features = binary_cols[chi2_selector.get_support()]\n",
    "\n",
    "print(\"\\n--- 3. Chi-Square Selected Features (Top 10 Binary) ---\")\n",
    "print(chi2_selected_features.tolist())\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 4. FINAL SELECTION & TRANSFORMATION\n",
    "# =========================================================================\n",
    "\n",
    "# For simplicity and robustness, we will select the top 12 features based on the\n",
    "# Random Forest Importance scores (a highly reliable model-based method).\n",
    "\n",
    "# Get the names of the final selected features (e.g., top 12)\n",
    "final_selected_features = importance_df.head(12).index.tolist()\n",
    "\n",
    "# Create the final feature-selected datasets\n",
    "X_train_fs = X_train[final_selected_features]\n",
    "X_test_fs = X_test[final_selected_features]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
