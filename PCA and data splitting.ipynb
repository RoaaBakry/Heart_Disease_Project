{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0ddad5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, classification_report\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# =========================================================================\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# 3. DATA SPLITTING\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# =========================================================================\u001b[39;00m\n\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Convert target y to a 1D array (required by train_test_split)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m y_flat = \u001b[43my\u001b[49m.values.ravel()\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Split data into 80% training and 20% testing sets\u001b[39;00m\n\u001b[32m     19\u001b[39m X_train, X_test, y_train, y_test = train_test_split(\n\u001b[32m     20\u001b[39m     X_encoded, y_flat, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m, stratify=y_flat\n\u001b[32m     21\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# =========================================================================\n",
    "# 3. DATA SPLITTING\n",
    "# =========================================================================\n",
    "\n",
    "# Convert target y to a 1D array (required by train_test_split)\n",
    "y_flat = y.values.ravel()\n",
    "\n",
    "# Split data into 80% training and 20% testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y_flat, test_size=0.2, random_state=42, stratify=y_flat\n",
    ")\n",
    "\n",
    "print(f\"Data Split: X_train shape {X_train.shape}, X_test shape {X_test.shape}\")\n",
    "\n",
    "# =========================================================================\n",
    "# 4. VISUALIZATION AND PCA ANALYSIS\n",
    "\n",
    "#1. Prepare Data for Correlation: Combine features and the target\n",
    "df_corr_viz = X_encoded.copy()\n",
    "df_corr_viz['target'] = y.values.ravel() # Add the target variable as a 1D array\n",
    "\n",
    "# 2. Calculate the correlation of all columns with the 'target' column\n",
    "# This gives us a single column of correlation coefficients\n",
    "target_corr = df_corr_viz.corr()[['target']].sort_values(by='target', ascending=False)\n",
    "\n",
    "# 3. Visualize the correlation\n",
    "plt.figure(figsize=(8, 15)) # Set the size for a vertical, readable plot\n",
    "sns.heatmap(\n",
    "    target_corr,\n",
    "    annot=True,          # **annot=True**: Display the correlation coefficient number inside each cell.\n",
    "    cmap='coolwarm',     # **cmap='coolwarm'**: Color map where red/warm colors show positive correlation (closer to +1) and blue/cool colors show negative correlation (closer to -1).\n",
    "    fmt=\".2f\",           # **fmt=\".2f\"**: Format the displayed numbers to two decimal places.\n",
    "    linewidths=.5,       # Add small white lines to separate the cells.\n",
    "    cbar=False           # Do not display the color bar since we are only visualizing one column of correlations.\n",
    ")\n",
    "plt.title('Feature Correlation with Heart Disease Target', fontsize=16)\n",
    "plt.yticks(rotation=0) # Keep feature names horizontal for easy reading\n",
    "plt.show()\n",
    "\n",
    "# Print the values directly for analysis\n",
    "print(\"\\n--- Top Correlation Values with Target ---\")\n",
    "print(target_corr.head(10))\n",
    "\n",
    "# 3. Distribution Plots for Numerical Features (Age)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(data=df_corr_viz, x='age', kde=True, bins=20, color='darkgreen')\n",
    "plt.title('Standardized Age Distribution', fontsize=16)\n",
    "plt.xlabel('Age (Scaled)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 4. Boxplot for Numerical Features (Focused View)\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.boxplot(data=df_corr_viz[numerical_cols], orient='h', palette='Set2')\n",
    "plt.title('Boxplot of Standardized Numerical Features', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# --- B. PCA Analysis to Find Optimal Components ---\n",
    "pca_full = PCA(n_components=None, random_state=42)\n",
    "pca_full.fit(X_train) # Fit on training data\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "n_components_95 = np.where(cumulative_variance >= 0.95)[0][0] + 1\n",
    "print(f\"Optimal number of components to retain 95% variance: {n_components_95}\")\n",
    "\n",
    "# --- C. Cumulative Explained Variance Plot ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--', color='blue')\n",
    "plt.axhline(y=0.95, color='r', linestyle='-', label='95% Cutoff')\n",
    "plt.axvline(x=n_components_95, color='g', linestyle='--', label=f'Optimal {n_components_95} Components')\n",
    "plt.title('Cumulative Explained Variance by Principal Components', fontsize=14)\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# --- D. PC1 vs PC2 Scatter Plot ---\n",
    "pca_2 = PCA(n_components=2, random_state=42)\n",
    "X_train_pca_2 = pca_2.fit_transform(X_train)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    x=X_train_pca_2[:, 0],\n",
    "    y=X_train_pca_2[:, 1],\n",
    "    hue=y_train,\n",
    "    palette='viridis',\n",
    "    legend='full'\n",
    ")\n",
    "plt.title('Visualization of Training Data in PC1 vs PC2 Space', fontsize=14)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
